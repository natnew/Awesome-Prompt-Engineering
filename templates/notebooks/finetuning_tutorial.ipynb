{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-tuning LLMs with LoRA (Low-Rank Adaptation)\n",
                "\n",
                "<a href=\"https://colab.research.google.com/github/natnew/Awesome-Prompt-Engineering/blob/main/templates/notebooks/finetuning_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "\n",
                "This notebook demonstrates how to fine-tune a small language model using **PEFT (Parameter-Efficient Fine-Tuning)** and **LoRA**. This approach allows you to train models on consumer hardware (or Google Colab's free T4 GPU) by only updating a small fraction of parameters.\n",
                "\n",
                "## Conceptual Overview\n",
                "1. **Load Base Model:** We'll use a small model (e.g., GPT-2 or TinyLlama) for demonstration.\n",
                "2. **Prepare Data:** Format text for instruction tuning.\n",
                "3. **Apply LoRA:** Inject trainable rank decomposition matrices into the model.\n",
                "4. **Train:** Run the training loop.\n",
                "5. **Save:** Save the adapter weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (uncomment if running in Colab)\n",
                "# !pip install transformers peft datasets bitsandbytes accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "from datasets import Dataset\n",
                "\n",
                "# 1. Load Model & Tokenizer\n",
                "model_id = \"gpt2\" # Using GPT-2 for speed and broad compatibility in this demo\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
                "\n",
                "# 2. Configure LoRA\n",
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM, \n",
                "    inference_mode=False, \n",
                "    r=8,            # Rank\n",
                "    lora_alpha=32,  # Alpha scaling\n",
                "    lora_dropout=0.1\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, peft_config)\n",
                "model.print_trainable_parameters()\n",
                "# You should see that only a tiny % of params are trainable!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare Dummy Dataset\n",
                "For this demo, we'll create a tiny dataset to show the format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = [\n",
                "    {\"text\": \"Question: What is RAG? Answer: Retrieval Augmented Generation combining search with LLMs.\"},\n",
                "    {\"text\": \"Question: What is LoRA? Answer: Low-Rank Adaptation for efficient fine-tuning.\"},\n",
                "    {\"text\": \"Question: Who is the author? Answer: The Awesome Prompt Engineering community.\"}\n",
                "]\n",
                "\n",
                "dataset = Dataset.from_list(data)\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
                "\n",
                "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
                "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train\n",
                "We use the Hugging Face `Trainer` to handle the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    per_device_train_batch_size=4,\n",
                "    num_train_epochs=3,\n",
                "    learning_rate=2e-4,\n",
                "    logging_steps=1,\n",
                "    use_cpu=True # Set to False if you have a GPU\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_datasets,\n",
                "    data_collator=data_collator,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Inference\n",
                "After training, we can use the model to generate text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "inputs = tokenizer(\"Question: What is LoRA? Answer:\", return_tensors=\"pt\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=20)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}