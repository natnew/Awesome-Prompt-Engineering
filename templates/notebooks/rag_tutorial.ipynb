{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction to RAG (Retrieval Augmented Generation)\n",
                "\n",
                "Retrieval Augmented Generation (RAG) combines the power of LLMs with external knowledge retrieval. This allows the model to answer questions about data it wasn't trained on (like your private documents).\n",
                "\n",
                "## The Flow\n",
                "1. **Chunking:** Break documents into smaller pieces.\n",
                "2. **Embedding:** Convert chunks into vector representations (numbers).\n",
                "3. **Retrieval:** Find the most similar chunks to a user's question.\n",
                "4. **Generation:** Pass the retrieved context + question to the LLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "# 1. THE KNOWLEDGE BASE (Mock Data)\n",
                "# In a real app, this would be your PDF/Database content\n",
                "documents = [\n",
                "    \"RAG stands for Retrieval Augmented Generation.\",\n",
                "    \"Fine-tuning updates the model weights, while RAG adds context at inference time.\",\n",
                "    \"Vector databases are commonly used to store embeddings for RAG systems.\",\n",
                "    \"Prompt engineering focuses on optimizing instructions sent to the model.\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2 & 3. Embedding and Retrieval (Simplified)\n",
                "\n",
                "In production, you'd use OpenAI embeddings (`text-embedding-3-small`) or HuggingFace models. Here, we'll use a simple keyword overlap to demonstrate the *concept* of finding relevant chunks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simple_retrieve(query, docs):\n",
                "    query_terms = set(query.lower().split())\n",
                "    scores = []\n",
                "    \n",
                "    for doc in docs:\n",
                "        doc_terms = set(doc.lower().split())\n",
                "        # Simple Jaccard similarity\n",
                "        intersection = query_terms.intersection(doc_terms)\n",
                "        score = len(intersection) / len(query_terms.union(doc_terms))\n",
                "        scores.append((score, doc))\n",
                "    \n",
                "    # Sort by score desc\n",
                "    scores.sort(key=lambda x: x[0], reverse=True)\n",
                "    return scores[0][1] # Return best match\n",
                "\n",
                "query = \"How is RAG different from fine-tuning?\"\n",
                "best_context = simple_retrieve(query, documents)\n",
                "\n",
                "print(f\"Query: {query}\")\n",
                "print(f\"Retrieved Context: {best_context}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generation\n",
                "\n",
                "Now we combine the context and question into a prompt for the LLM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_rag_prompt(query, context):\n",
                "    return f\"\"\"\n",
                "Context Information:\n",
                "{context}\n",
                "\n",
                "Question: {query}\n",
                "\n",
                "Instructions: Answer the question using ONLY the provided context information. \n",
                "If the answer is not in the context, say \"I don't know.\"\n",
                "\"\"\"\n",
                "\n",
                "final_prompt = generate_rag_prompt(query, best_context)\n",
                "print(\"--- Final Prompt Sent to LLM ---\")\n",
                "print(final_prompt)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Next Steps\n",
                "\n",
                "For a production-grade implementation including proper evaluation metrics (Precision, Recall, Drift), check out our **Core RAG Evaluation Project** in this repository:\n",
                "\n",
                "`projects/core/01_rag_evaluation_pipeline`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}